{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "import avro.schema\n",
    "import avro.io\n",
    "import io\n",
    "\n",
    "schema = avro.schema.parse(\"\"\"{\"namespace\": \"com.cisco.pnda\",\n",
    " \"type\": \"record\",\n",
    " \"name\": \"PndaRecord\",\n",
    " \"fields\": [\n",
    "     {\"name\": \"timestamp\",   \"type\": \"long\"},\n",
    "     {\"name\": \"src\",         \"type\": \"string\"},\n",
    "     {\"name\": \"host_ip\",     \"type\": \"string\"},\n",
    "     {\"name\": \"rawdata\",     \"type\": \"bytes\"}\n",
    " ]\n",
    "}\"\"\")\n",
    "\n",
    "reader = avro.io.DatumReader(schema)\n",
    "\n",
    "ssc = StreamingContext(sc, 10)\n",
    "\n",
    "def avroDecoder(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    byte_reader = io.BytesIO(s)\n",
    "    decoder = avro.io.BinaryDecoder(byte_reader)\n",
    "    return reader.read(decoder)\n",
    "\n",
    "kafkaParams = {\"metadata.broker.list\": \"192.168.56.101:9092\", \"auto.offset.reset\": \"largest\"}\n",
    "    \n",
    "topic = \"collectd\"\n",
    "\n",
    "kafkaStream = KafkaUtils.createDirectStream(ssc,[topic],kafkaParams, valueDecoder=avroDecoder)\n",
    "\n",
    "kafka_rdd = kafkaStream.map(lambda x:x[1])\n",
    "\n",
    "def transform(rdd):\n",
    "    \n",
    "    def openTsdbApiPut(data):\n",
    "        host = 'localhost:4242'\n",
    "        openTsdbUrl = 'http://' + host + '/api/put/details'\n",
    "        r = requests.post(openTsdbUrl, data = json.dumps(data))\n",
    "        r.raise_for_status()\n",
    "\n",
    "    items = rdd.collect()\n",
    "    for item in items:\n",
    "        message = json.loads(item['rawdata'])\n",
    "        data = []\n",
    "        for name in 'inPkts', 'outPkts':\n",
    "            metric = {\n",
    "                \"metric\": name,\n",
    "                \"timestamp\": item['timestamp'],\n",
    "                \"value\": message[name],\n",
    "                \"tags\": {\n",
    "                    \"node\": item['host_ip'],\n",
    "                    \"interface\": message['interface']\n",
    "                }\n",
    "            }\n",
    "            data.append(metric)\n",
    "        openTsdbApiPut(data)\n",
    "    if len(items) > 0:\n",
    "        print \"Stored metrics for {} samples\".format(len(items))\n",
    "\n",
    "kafka_rdd.foreachRDD(lambda rdd: transform(rdd))\n",
    "\n",
    "ssc.start()\n",
    "ssc.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
